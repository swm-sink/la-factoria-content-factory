# SLA Alerting Configuration
# Defines alert rules, thresholds, and notification channels

alert_rules:
  # API Availability Alerts
  api_availability_critical:
    name: "API Availability Critical"
    condition: "api_availability < 99.5"
    duration: "5m"
    severity: "critical"
    channels:
      - pagerduty
      - slack_critical
    message: "API availability dropped below 99.5% (current: {{ $value }}%)"
    
  api_availability_warning:
    name: "API Availability Warning"
    condition: "api_availability < 99.8"
    duration: "10m"
    severity: "warning"
    channels:
      - slack_engineering
    message: "API availability below target (current: {{ $value }}%)"

  # Error Budget Alerts
  error_budget_critical:
    name: "Error Budget Critical"
    condition: "error_budget_consumption_rate > 90"
    duration: "1m"
    severity: "critical"
    channels:
      - pagerduty
      - slack_critical
    message: "Error budget nearly exhausted ({{ $value }}% consumed)"
    
  error_budget_high_burn:
    name: "Error Budget High Burn Rate"
    condition: "error_budget_burn_rate > 5"
    duration: "5m"
    severity: "error"
    channels:
      - slack_engineering
      - email_oncall
    message: "Error budget burning {{ $value }}x faster than normal"

  # Latency Alerts
  latency_p95_violation:
    name: "P95 Latency SLO Violation"
    condition: "api_latency_p95 > 500"
    duration: "5m"
    severity: "warning"
    channels:
      - slack_engineering
    message: "P95 latency exceeds SLO (current: {{ $value }}ms)"
    
  latency_p99_critical:
    name: "P99 Latency Critical"
    condition: "api_latency_p99 > 2000"
    duration: "3m"
    severity: "critical"
    channels:
      - pagerduty
      - slack_critical
    message: "P99 latency critically high (current: {{ $value }}ms)"

  # Error Rate Alerts
  high_error_rate:
    name: "High Error Rate"
    condition: "error_rate > 2"
    duration: "5m"
    severity: "error"
    channels:
      - slack_engineering
      - email_oncall
    message: "Error rate exceeds 2% (current: {{ $value }}%)"
    
  critical_error_spike:
    name: "Critical Error Spike"
    condition: "error_rate > 5"
    duration: "1m"
    severity: "critical"
    channels:
      - pagerduty
      - slack_critical
    message: "Critical error spike detected ({{ $value }}% error rate)"

  # Content Generation Alerts
  content_generation_failure:
    name: "Content Generation Failures"
    condition: "content_generation_success_rate < 95"
    duration: "10m"
    severity: "error"
    channels:
      - slack_engineering
    message: "Content generation success rate low ({{ $value }}%)"

  # Database Performance Alerts
  database_latency_high:
    name: "Database Latency High"
    condition: "database_query_p95 > 100"
    duration: "5m"
    severity: "warning"
    channels:
      - slack_engineering
    message: "Database query latency high (P95: {{ $value }}ms)"
    
  database_connection_pool_exhausted:
    name: "Database Connection Pool Exhausted"
    condition: "database_connection_pool_usage > 90"
    duration: "3m"
    severity: "error"
    channels:
      - slack_engineering
      - email_oncall
    message: "Database connection pool nearly exhausted ({{ $value }}% used)"

  # Dependency Alerts
  dependency_unhealthy:
    name: "Service Dependency Unhealthy"
    condition: "dependency_health != 'healthy'"
    duration: "5m"
    severity: "error"
    channels:
      - slack_engineering
    message: "Service dependency {{ $labels.service }} is {{ $value }}"

# Notification Channels
notification_channels:
  pagerduty:
    type: "pagerduty"
    routing_key: "${PAGERDUTY_ROUTING_KEY}"
    
  slack_critical:
    type: "slack"
    webhook_url: "${SLACK_CRITICAL_WEBHOOK}"
    channel: "#alerts-critical"
    
  slack_engineering:
    type: "slack"
    webhook_url: "${SLACK_ENGINEERING_WEBHOOK}"
    channel: "#engineering-alerts"
    
  email_oncall:
    type: "email"
    to: "oncall@lafactoria.ai"
    from: "alerts@lafactoria.ai"

# Alert Aggregation Rules
aggregation_rules:
  # Group related alerts
  latency_issues:
    alerts:
      - latency_p95_violation
      - latency_p99_critical
    group_by: ["endpoint"]
    group_wait: "30s"
    group_interval: "5m"
    
  error_budget_issues:
    alerts:
      - error_budget_critical
      - error_budget_high_burn
    group_by: ["slo"]
    group_wait: "1m"
    group_interval: "5m"

# Inhibition Rules
inhibition_rules:
  # Don't alert on latency if availability is down
  - source_match:
      alertname: "api_availability_critical"
    target_match:
      alertname: "latency_.*"
    equal: ["service"]
    
  # Don't alert on individual errors during error spike
  - source_match:
      alertname: "critical_error_spike"
    target_match:
      alertname: "high_error_rate"
    equal: ["service"]

# Silences (maintenance windows)
silence_rules:
  scheduled_maintenance:
    match:
      alertname: ".*"
    schedule:
      - day: "tuesday"
        start: "02:00"
        end: "04:00"
        timezone: "UTC"
    comment: "Weekly maintenance window"

# Escalation Policies
escalation_policies:
  default:
    levels:
      - delay: "0m"
        targets:
          - slack_engineering
      - delay: "5m"
        targets:
          - email_oncall
      - delay: "15m"
        targets:
          - pagerduty
          
  critical:
    levels:
      - delay: "0m"
        targets:
          - pagerduty
          - slack_critical
      - delay: "5m"
        targets:
          - escalate_to_manager

# Alert Templates
templates:
  default_alert:
    title: "{{ .GroupLabels.alertname }} - {{ .GroupLabels.severity }}"
    text: |
      Summary: {{ .CommonAnnotations.summary }}
      Description: {{ .CommonAnnotations.description }}
      
      Alerts:
      {{ range .Alerts }}
      - {{ .Labels.alertname }}: {{ .Annotations.message }}
        Started: {{ .StartsAt }}
        Value: {{ .Labels.value }}
      {{ end }}
      
      [View Dashboard](https://lafactoria.ai/admin/sla)
      
  slack_alert:
    attachments:
      - color: "{{ if eq .GroupLabels.severity \"critical\" }}danger{{ else if eq .GroupLabels.severity \"error\" }}warning{{ else }}good{{ end }}"
        title: "{{ .GroupLabels.alertname }}"
        text: "{{ .CommonAnnotations.message }}"
        fields:
          - title: "Severity"
            value: "{{ .GroupLabels.severity }}"
            short: true
          - title: "Service"
            value: "{{ .GroupLabels.service }}"
            short: true
        footer: "La Factoria Monitoring"
        ts: "{{ .Timestamp }}"