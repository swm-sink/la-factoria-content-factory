name: End-to-End Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to test against'
        required: true
        default: 'staging'
        type: choice
        options:
        - staging
        - production

env:
  PYTHON_VERSION: '3.11'

jobs:
  e2e-tests:
    name: E2E Tests
    runs-on: ubuntu-latest
    environment: ${{ github.event.inputs.environment || 'staging' }}

    strategy:
      matrix:
        test-suite:
          - content-generation
          - error-handling
          - performance

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt

    - name: Wait for deployment (if triggered by deployment)
      if: github.event_name == 'deployment_status'
      run: |
        echo "Waiting for deployment to be ready..."
        sleep 60

    - name: Health check target environment
      env:
        E2E_BASE_URL: ${{ vars.E2E_BASE_URL || 'https://staging-ai-content-factory.cloudfunctions.net' }}
        E2E_API_KEY: ${{ secrets.E2E_API_KEY }}
      run: |
        python -c "
        import requests
        import sys
        import os

        base_url = os.getenv('E2E_BASE_URL')
        api_key = os.getenv('E2E_API_KEY')

        try:
            response = requests.get(f'{base_url}/health',
                                  headers={'X-API-Key': api_key},
                                  timeout=30)
            if response.status_code == 200:
                print(f'âœ… Service health check passed: {base_url}')
            else:
                print(f'âŒ Health check failed: {response.status_code}')
                sys.exit(1)
        except Exception as e:
            print(f'âŒ Health check error: {e}')
            sys.exit(1)
        "

    - name: Run E2E Tests - Content Generation
      if: matrix.test-suite == 'content-generation'
      env:
        E2E_BASE_URL: ${{ vars.E2E_BASE_URL || 'https://staging-ai-content-factory.cloudfunctions.net' }}
        E2E_API_KEY: ${{ secrets.E2E_API_KEY }}
        PYTEST_TIMEOUT: 300
      run: |
        python -m pytest tests/e2e/test_content_generation_e2e.py::ContentGenerationE2ETest::test_complete_content_generation_workflow \
          -v --tb=short --timeout=$PYTEST_TIMEOUT \
          --junitxml=test-results-content-generation.xml

    - name: Run E2E Tests - Error Handling
      if: matrix.test-suite == 'error-handling'
      env:
        E2E_BASE_URL: ${{ vars.E2E_BASE_URL || 'https://staging-ai-content-factory.cloudfunctions.net' }}
        E2E_API_KEY: ${{ secrets.E2E_API_KEY }}
        PYTEST_TIMEOUT: 180
      run: |
        python -m pytest tests/e2e/test_content_generation_e2e.py::ContentGenerationE2ETest::test_error_handling_and_recovery \
          -v --tb=short --timeout=$PYTEST_TIMEOUT \
          --junitxml=test-results-error-handling.xml

    - name: Run E2E Tests - Performance & Caching
      if: matrix.test-suite == 'performance'
      env:
        E2E_BASE_URL: ${{ vars.E2E_BASE_URL || 'https://staging-ai-content-factory.cloudfunctions.net' }}
        E2E_API_KEY: ${{ secrets.E2E_API_KEY }}
        PYTEST_TIMEOUT: 240
      run: |
        python -m pytest tests/e2e/test_content_generation_e2e.py::ContentGenerationE2ETest::test_caching_behavior \
          tests/e2e/test_content_generation_e2e.py::ContentGenerationE2ETest::test_rate_limiting_behavior \
          -v --tb=short --timeout=$PYTEST_TIMEOUT \
          --junitxml=test-results-performance.xml

    - name: Generate E2E Test Report
      if: always()
      env:
        E2E_BASE_URL: ${{ vars.E2E_BASE_URL || 'https://staging-ai-content-factory.cloudfunctions.net' }}
        E2E_API_KEY: ${{ secrets.E2E_API_KEY }}
      run: |
        python tests/e2e/test_content_generation_e2e.py \
          --base-url="$E2E_BASE_URL" \
          --api-key="$E2E_API_KEY" \
          --output="e2e-results-${{ matrix.test-suite }}.json" || true

    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: e2e-test-results-${{ matrix.test-suite }}
        path: |
          test-results-*.xml
          e2e-results-*.json
        retention-days: 30

    - name: Comment PR with E2E Results
      if: github.event_name == 'pull_request' && always()
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = require('path');

          let comment = `## ðŸ§ª E2E Test Results - ${{ matrix.test-suite }}\n\n`;

          try {
            const resultFile = `e2e-results-${{ matrix.test-suite }}.json`;
            if (fs.existsSync(resultFile)) {
              const results = JSON.parse(fs.readFileSync(resultFile, 'utf8'));

              comment += `**Environment**: ${{ vars.E2E_BASE_URL || 'staging' }}\n`;
              comment += `**Tests Run**: ${results.tests_run}\n`;
              comment += `**Passed**: ${results.tests_passed} âœ…\n`;
              comment += `**Failed**: ${results.tests_failed} ${results.tests_failed > 0 ? 'âŒ' : 'âœ…'}\n`;
              comment += `**Success Rate**: ${results.success_rate.toFixed(1)}%\n`;
              comment += `**Duration**: ${results.total_duration}s\n\n`;

              if (results.test_results && results.test_results.length > 0) {
                comment += `### Test Details\n\n`;
                results.test_results.forEach(test => {
                  const status = test.status === 'PASSED' ? 'âœ…' : 'âŒ';
                  comment += `- ${status} **${test.name}** (${test.duration}s)\n`;
                  if (test.error) {
                    comment += `  - Error: ${test.error}\n`;
                  }
                });
              }
            } else {
              comment += `âŒ Test results file not found\n`;
            }
          } catch (error) {
            comment += `âŒ Error reading test results: ${error.message}\n`;
          }

          // Find existing comment and update or create new
          const { data: comments } = await github.rest.issues.listComments({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number,
          });

          const existingComment = comments.find(c =>
            c.body.includes(`E2E Test Results - ${{ matrix.test-suite }}`)
          );

          if (existingComment) {
            await github.rest.issues.updateComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: existingComment.id,
              body: comment
            });
          } else {
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: comment
            });
          }

  consolidate-results:
    name: Consolidate E2E Results
    runs-on: ubuntu-latest
    needs: e2e-tests
    if: always()

    steps:
    - name: Download all test artifacts
      uses: actions/download-artifact@v3
      with:
        path: test-results

    - name: Consolidate test results
      run: |
        echo "## ðŸ“Š E2E Test Summary" >> summary.md
        echo "" >> summary.md

        total_tests=0
        total_passed=0
        total_failed=0

        for suite in content-generation error-handling performance; do
          if [ -f "test-results/e2e-test-results-${suite}/e2e-results-${suite}.json" ]; then
            echo "Processing ${suite} results..."

            # Parse JSON results (would need jq in real scenario, using simplified approach)
            echo "### ${suite^} Test Suite" >> summary.md
            echo "Results available in artifacts" >> summary.md
            echo "" >> summary.md
          fi
        done

        cat summary.md

    - name: Upload consolidated summary
      uses: actions/upload-artifact@v3
      with:
        name: e2e-test-summary
        path: summary.md
        retention-days: 30

  notify-on-failure:
    name: Notify on E2E Failure
    runs-on: ubuntu-latest
    needs: e2e-tests
    if: failure() && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop')

    steps:
    - name: Notify team of E2E failure
      env:
        SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
      run: |
        if [ -n "$SLACK_WEBHOOK" ]; then
          curl -X POST -H 'Content-type: application/json' \
            --data '{"text":"ðŸš¨ AI Content Factory E2E Tests Failed on '"${{ github.ref }}"'\nCommit: '"${{ github.sha }}"'\nWorkflow: '"${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"'"}' \
            $SLACK_WEBHOOK
        else
          echo "No Slack webhook configured for notifications"
        fi
