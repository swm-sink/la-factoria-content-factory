name: Load Testing

on:
  workflow_dispatch:
    inputs:
      host:
        description: 'Target host URL'
        required: true
        default: 'http://localhost:8000'
        type: string
      users:
        description: 'Number of concurrent users'
        required: true
        default: '50'
        type: string
      spawn_rate:
        description: 'Users spawn rate (users/second)'
        required: true
        default: '5'
        type: string
      duration:
        description: 'Test duration (e.g., 5m, 300s)'
        required: true
        default: '5m'
        type: string
      scenario:
        description: 'Test scenario to run'
        required: true
        default: 'main'
        type: choice
        options:
          - main
          - authentication
          - content_generation
          - stress_test
      environment:
        description: 'Target environment'
        required: true
        default: 'staging'
        type: choice
        options:
          - local
          - staging
          - production

permissions:
  contents: read
  pull-requests: write

env:
  PYTHON_VERSION: "3.11"

jobs:
  load-test:
    name: Run Load Test
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Cache Python dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: pip-loadtest-${{ runner.os }}-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            pip-loadtest-${{ runner.os }}-
            
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install locust
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          
      - name: Set environment variables
        run: |
          echo "Setting up environment for ${{ github.event.inputs.environment }}..."
          if [ "${{ github.event.inputs.environment }}" == "production" ]; then
            echo "LA_FACTORIA_API_KEY=${{ secrets.PROD_API_KEY }}" >> $GITHUB_ENV
            echo "API_HOST=${{ secrets.PROD_API_URL }}" >> $GITHUB_ENV
          elif [ "${{ github.event.inputs.environment }}" == "staging" ]; then
            echo "LA_FACTORIA_API_KEY=${{ secrets.STAGING_API_KEY }}" >> $GITHUB_ENV
            echo "API_HOST=${{ secrets.STAGING_API_URL }}" >> $GITHUB_ENV
          else
            echo "LA_FACTORIA_API_KEY=test-api-key-12345" >> $GITHUB_ENV
            echo "API_HOST=${{ github.event.inputs.host }}" >> $GITHUB_ENV
          fi
          
      - name: Validate load test setup
        run: |
          echo "Validating load test setup..."
          python scripts/validate_load_testing.py || echo "Validation script not critical for CI"
          
      - name: Select test file
        id: select-test
        run: |
          case "${{ github.event.inputs.scenario }}" in
            "authentication")
              echo "TEST_FILE=tests/performance/scenarios/authentication_scenario.py" >> $GITHUB_OUTPUT
              ;;
            "content_generation")
              echo "TEST_FILE=tests/performance/scenarios/content_generation_scenario.py" >> $GITHUB_OUTPUT
              ;;
            "stress_test")
              echo "TEST_FILE=tests/performance/scenarios/stress_test_scenario.py" >> $GITHUB_OUTPUT
              ;;
            *)
              echo "TEST_FILE=tests/performance/locustfile.py" >> $GITHUB_OUTPUT
              ;;
          esac
          
      - name: Run load test
        run: |
          echo "Running load test..."
          echo "Target: $API_HOST"
          echo "Users: ${{ github.event.inputs.users }}"
          echo "Duration: ${{ github.event.inputs.duration }}"
          echo "Scenario: ${{ github.event.inputs.scenario }}"
          
          locust \
            -f ${{ steps.select-test.outputs.TEST_FILE }} \
            --host=$API_HOST \
            --users=${{ github.event.inputs.users }} \
            --spawn-rate=${{ github.event.inputs.spawn_rate }} \
            --run-time=${{ github.event.inputs.duration }} \
            --headless \
            --html=load_test_report.html \
            --csv=results \
            --csv-full-history \
            --loglevel=INFO
            
      - name: Generate performance report
        if: always()
        run: |
          echo "## Load Test Results" > performance_report.md
          echo "" >> performance_report.md
          echo "### Test Configuration" >> performance_report.md
          echo "- **Environment**: ${{ github.event.inputs.environment }}" >> performance_report.md
          echo "- **Target**: $API_HOST" >> performance_report.md
          echo "- **Users**: ${{ github.event.inputs.users }}" >> performance_report.md
          echo "- **Spawn Rate**: ${{ github.event.inputs.spawn_rate }} users/s" >> performance_report.md
          echo "- **Duration**: ${{ github.event.inputs.duration }}" >> performance_report.md
          echo "- **Scenario**: ${{ github.event.inputs.scenario }}" >> performance_report.md
          echo "" >> performance_report.md
          
          if [ -f results_stats.csv ]; then
            echo "### Performance Summary" >> performance_report.md
            echo '```' >> performance_report.md
            head -20 results_stats.csv >> performance_report.md
            echo '```' >> performance_report.md
          fi
          
      - name: Check performance thresholds
        if: github.event.inputs.environment != 'production'
        run: |
          echo "Checking performance against baselines..."
          python -c "
import json
import csv
import sys

# Load baseline
with open('tests/performance/baseline.json') as f:
    baseline = json.load(f)

# Check if results exist
try:
    with open('results_stats.csv') as f:
        reader = csv.DictReader(f)
        stats = list(reader)
except FileNotFoundError:
    print('No results file found, skipping threshold check')
    sys.exit(0)

# Basic threshold check
failures = []
for stat in stats:
    if stat['Type'] == 'Request':
        name = stat['Name']
        p95 = float(stat['95%'] or 0)
        error_rate = float(stat['Failure Count'] or 0) / float(stat['Request Count'] or 1)
        
        # Check against baseline if exists
        for endpoint, baseline_data in baseline['endpoints'].items():
            if endpoint in name:
                expected_p95 = baseline_data['expected_response_time_ms']['p95']
                if p95 > expected_p95 * 1.5:  # Allow 50% margin
                    failures.append(f'{name}: P95 {p95:.0f}ms exceeds baseline {expected_p95}ms')
                if error_rate > baseline_data['expected_error_rate'] * 2:  # Allow 2x margin
                    failures.append(f'{name}: Error rate {error_rate:.2%} exceeds baseline')

if failures:
    print('Performance threshold violations:')
    for failure in failures:
        print(f'  - {failure}')
    sys.exit(1)
else:
    print('All performance thresholds passed!')
"
          
      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: load-test-results-${{ github.run_number }}
          path: |
            load_test_report.html
            results*.csv
            performance_report.md
          retention-days: 30
          
      - name: Comment on PR (if applicable)
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('performance_report.md', 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });
            
      - name: Send notification (production tests)
        if: github.event.inputs.environment == 'production' && failure()
        run: |
          echo "⚠️ Production load test failed!"
          echo "Check the artifacts for detailed results"
          # Add notification logic here (Slack, email, etc.)

  analyze-results:
    name: Analyze Load Test Results
    runs-on: ubuntu-latest
    needs: load-test
    if: always()
    
    steps:
      - name: Download test results
        uses: actions/download-artifact@v3
        with:
          name: load-test-results-${{ github.run_number }}
          
      - name: Generate analysis summary
        run: |
          echo "## Load Test Analysis" > analysis.md
          echo "" >> analysis.md
          echo "### Key Findings" >> analysis.md
          
          # Add more sophisticated analysis here
          if [ -f results_stats.csv ]; then
            python -c "
import csv
import statistics

with open('results_stats.csv') as f:
    reader = csv.DictReader(f)
    stats = [s for s in reader if s['Type'] == 'Request']

if stats:
    # Calculate overall metrics
    total_requests = sum(int(s['Request Count'] or 0) for s in stats)
    total_failures = sum(int(s['Failure Count'] or 0) for s in stats)
    overall_error_rate = total_failures / total_requests if total_requests > 0 else 0
    
    # Find slowest endpoints
    slowest = sorted(stats, key=lambda s: float(s['95%'] or 0), reverse=True)[:5]
    
    print(f'- Total Requests: {total_requests:,}')
    print(f'- Overall Error Rate: {overall_error_rate:.2%}')
    print('')
    print('Top 5 Slowest Endpoints (P95):')
    for s in slowest:
        print(f'  - {s[\"Name\"]}: {float(s[\"95%\"] or 0):.0f}ms')
"
          fi >> analysis.md
          
      - name: Upload analysis
        uses: actions/upload-artifact@v3
        with:
          name: load-test-analysis-${{ github.run_number }}
          path: analysis.md
          retention-days: 30